# syntax=docker/dockerfile:1

FROM python:3.11-slim-bookworm AS base

ARG VLLM_VERSION=0.7.2
ARG EFA_INSTALLER_VERSION=1.37.0

# Neuron SDK components version numbers
ARG TORCH_VERSION=2.5.1
ARG TORCH_NEURONX_VERSION=${TORCH_VERSION}.2.4.0
ARG NEURONX_TRANSFORMERS_VERSION=0.13.380
ARG NEURONX_COLLECTIVES_LIB_VERSION=2.23.135.0-3e70920f2
ARG NEURONX_RUNTIME_LIB_VERSION=2.23.112.0-9b5179492
ARG NEURONX_DISTRIBUTED_VERSION=0.10.1
ARG NEURONX_DISTRIBUTED_INFERENCE_VERSION=0.1.1

ARG PYTORCH_VERSION=2.5.1
ARG PIP_ARGS='--no-cache-dir'
# For Python 3.12+
# ARG PIP_ARGS='--no-cache-dir --break-system-packages'

ENV VLLM_TARGET_DEVICE=neuron
ENV PYTHONUNBUFFERED=1
ARG PIP_ROOT_USER_ACTION=ignore

RUN <<EOT
  rm -f /etc/apt/apt.conf.d/docker-clean
  echo 'Binary::apt::APT::Keep-Downloaded-Packages "true";' > /etc/apt/apt.conf.d/keep-cache
  echo 'APT::Install-Suggests "0";' >> /etc/apt/apt.conf.d/00-docker
  echo 'APT::Install-Recommends "0";' >> /etc/apt/apt.conf.d/00-docker
  echo 'debconf debconf/frontend select Noninteractive' | debconf-set-selections

    # Set up extra index URLs for Neuron and PyTorch CPU only wheels
  mkdir -p /root/.config/pip/
  cat <<-EOF > /root/.config/pip/pip.conf
[global]
extra-index-url = https://pip.repos.neuron.amazonaws.com
                  https://download.pytorch.org/whl/cpu/
EOF

  apt-get update
  apt-get install -y \
    apt-utils \
    ca-certificates \
    curl \
    gnupg2 \
    wget

  # Add Neuron apt repository
  curl -fsSL https://apt.repos.neuron.amazonaws.com/GPG-PUB-KEY-AMAZON-AWS-NEURON.PUB | gpg --dearmor -o /usr/share/keyrings/neuron.gpg
  echo "deb [arch=amd64 signed-by=/usr/share/keyrings/neuron.gpg] https://apt.repos.neuron.amazonaws.com jammy main" | tee /etc/apt/sources.list.d/neuron.list > /dev/null
  apt-get update

  apt-get remove -y \
    curl \
    wget
  apt-get autoremove -y
  apt-get clean
  find / -name '*.a' | xargs rm -f
  rm -rf /var/lib/apt/lists/*
EOT

###########################################################

FROM base AS neuron-vllm

RUN <<EOT
  apt-get update
  apt-get install -y \
    aws-neuronx-collectives=${NEURONX_COLLECTIVES_LIB_VERSION} \
    aws-neuronx-runtime-lib=${NEURONX_RUNTIME_LIB_VERSION}

  apt-get autoremove -y
  apt-get clean
  find / -name '*.a' | xargs rm -f
  rm -rf /var/lib/apt/lists/*
EOT

RUN <<EOT
  pip install $PIP_ARGS --upgrade pip --trusted-host pypi.org --trusted-host files.pythonhosted.org
  pip install $PIP_ARGS \
    torch==${TORCH_VERSION%\.}+cpu \
    torch-xla==${TORCH_VERSION} \
    torch-neuronx==${TORCH_NEURONX_VERSION} \
    transformers-neuronx==${NEURONX_TRANSFORMERS_VERSION}

  pip install $PIP_ARGS \
    neuronx_distributed==$NEURONX_DISTRIBUTED_VERSION \
    neuronx_distributed_inference==$NEURONX_DISTRIBUTED_INFERENCE_VERSION
EOT

# vLLM
RUN <<EOT
  apt-get update
  apt-get install -y \
    curl

  cd /tmp
  curl -sL https://github.com/vllm-project/vllm/releases/download/v${VLLM_VERSION}/vllm-${VLLM_VERSION}.tar.gz | tar xvz

  cd vllm-${VLLM_VERSION}
  pip install $PIP_ARGS \
    -r requirements-neuron.txt
  pip install $PIP_ARGS .
  cd /
  rm -rf /tmp/*

  # Remove un-needed packages (no images)
  pip uninstall $PIP_ARGS -y \
    airportsdata \
    pillow \
    pycountry \
    pytest

  apt-get remove -y \
    curl
  apt-get autoremove -y
  apt-get clean
  find / -name '*.a' | xargs rm -f
  rm -rf /var/lib/apt/lists/*
EOT

# EFA installer
RUN <<EOT
  apt-get update
  apt-get install -y \
    curl

  cd /tmp
  curl -sL https://efa-installer.amazonaws.com/aws-efa-installer-${EFA_INSTALLER_VERSION}.tar.gz | tar xvz
  cd aws-efa-installer

  ./efa_installer.sh --yes --skip-kmod --skip-limit-conf --no-verify --mpi openmpi4

  echo '/opt/amazon/openmpi/lib' > /etc/ld.so.conf.d/openmpi.conf
  ldconfig

  apt-get remove -y \
    curl
  apt-get autoremove -y
  apt-get clean
  find / -name '*.a' | xargs rm -f
  rm -rf /var/lib/apt/lists/*
EOT

# Will cause layer cache every time
RUN <<EOT
  apt-get update
  apt-get upgrade -y
  apt-get autoremove -y
  apt-get clean
  rm -rf /var/lib/apt/lists/*
EOT

#################### OPENAI API SERVER ####################

FROM neuron-vllm AS neuron-vllm-openai

RUN <<EOT
  pip install $PIP_ARGS \
    accelerate \
    hf_transfer \
    'modelscope!=1.15.0' \
    'bitsandbytes>=0.42.0' \
    'timm==0.9.10' \
    boto3 \
    runai-model-streamer \
    runai-model-streamer[s3]
EOT

# ENTRYPOINT ["python3", "-m", "vllm.entrypoints.openai.api_server"]
